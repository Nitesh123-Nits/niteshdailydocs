System design interviews**, using â€œloadedâ€ yet meaningful terms like _hot key_, _hot API_, _cold path_, _cache stampede_, _fan-out_, _thundering herd_, etc., conveys **depth and practical understanding** â€” not because they sound fancy, but because they hint that youâ€™ve faced real-world scale issues and understand _why systems behave the way they do under load_.

Letâ€™s unpack **â€œhot key,â€ â€œhot API,â€ and related terms** in a structured way â€” including meaning, examples, problems they cause, and how to handle them.

---

## ðŸ§  1. â€œHot Keyâ€ â€” Meaning & Context

**Definition:**  
A **hot key** refers to a _specific key or item_ that receives **disproportionately high access traffic** compared to others in a distributed system (cache, DB, or storage).

### ðŸ” Example:

- Suppose you have a **Redis cache** storing product details:
    
    ```
    product:{id} -> productDetails
    ```
    
    If `product:123` is an iPhone 16, and 80% of your users view that product page, then this key (`product:123`) becomes a **hot key**.
    

### âš ï¸ Problem:

- That single key gets **too many reads/writes** on one cache node or partition â†’ **hotspotting**, **load imbalance**, or even **throttling**.
    
- It breaks horizontal scalability assumptions (since load is uneven).
    

### ðŸ› ï¸ Solutions:

- **Sharding or Key Hashing**: Spread requests across nodes by random suffixing keys (e.g., `product:123:1`, `product:123:2`) and combine results.
    
- **Caching at Edge/CDN**: Offload popular key access closer to users.
    
- **Replication**: Store replicas for read-heavy keys.
    
- **Async batching**: Debounce or batch reads/writes on the same key.
    

### ðŸ’¬ When to use the word:

> â€œWe noticed a _hot key_ pattern in Redis due to one productâ€™s extreme popularity, which caused uneven load. We mitigated it by adding key randomization and read replicas.â€

That sounds **very seasoned and real-world**.

---

## ðŸŒ 2. â€œHot APIâ€ â€” Meaning & Context

**Definition:**  
A **hot API** (or **hot endpoint**) is an API endpoint that receives **a very high QPS (queries per second)** compared to other endpoints â€” becoming a **bottleneck or hotspot** in your microservice architecture.

### ðŸ” Example:

- In an e-commerce platform:
    
    - `/api/products/{id}` might be hit 100x more than `/api/users/{id}`
        
    - `/api/search` or `/api/homepage` could be _hot APIs_ since theyâ€™re called on every page load.
        

### âš ï¸ Problem:

- These endpoints experience **latency spikes**, **rate-limit issues**, or **backend contention**.
    
- Could trigger **DB load spikes** or **cache thrashing**.
    

### ðŸ› ï¸ Solutions:

- **Cache responses** (Redis, CDN, in-memory).
    
- **Rate limiting** and **load shedding**.
    
- **Read replicas** behind hot APIs.
    
- **Async pipelines or CQRS** (split read/write).
    
- **Edge caching** or **API Gateway-level caching**.
    

### ðŸ’¬ When to use the word:

> â€œThe `/api/homepage` endpoint turned into a _hot API_ after we went live â€” so we cached its response in Redis for 2 seconds to flatten load spikes.â€

That phrasing gives _instant credibility_.

---

## ðŸ§Š 3. â€œCold Keyâ€ or â€œCold Pathâ€

**Cold key** = rarely accessed data.  
**Cold path** = code path not frequently used (opposite of hot path).

Example:

- In analytics systems, todayâ€™s data is _hot_, last yearâ€™s is _cold_.  
    â†’ Hot data lives in Redis or SSDs; cold data in S3 or archive storage.
    

ðŸ’¬ Use case:

> â€œWe maintain a hot-cold data split where frequently accessed metrics stay in Redis and older metrics are moved to S3.â€

---

## âš¡ 4. â€œHot Partitionâ€ or â€œHot Shardâ€

If the load imbalance happens **at the partition level** (in Kafka, Cassandra, MongoDB, etc.), thatâ€™s called a **hot partition** or **hot shard**.

Example:

- Kafka topic has 10 partitions.
    
- Partition #3 gets 90% of writes â†’ consumers on that partition are overwhelmed.
    

ðŸ’¬ Use case:

> â€œWe faced a _hot partition_ issue in Kafka because our partitioning key wasnâ€™t uniform â€” we fixed it by using a composite key with random salt.â€

---

## ðŸ”¥ 5. â€œHot Pathâ€ or â€œHot Code Pathâ€

**Hot path** = the most frequently executed part of code in a request flow or microservice.

Example:

- In a service, the main request handler and database query path are â€œhot paths.â€
    
- Profiling and optimizing hot paths lead to big performance gains.
    

ðŸ’¬ Use case:

> â€œWe optimized the _hot path_ by switching from JSON to binary serialization, cutting 30% CPU overhead.â€

---

## ðŸ§© 6. Why These Words Matter

Using terms like:

- **Hot key**
    
- **Hot API**
    
- **Hot path**
    
- **Hot partition**
    
- **Cache stampede**
    
- **Thundering herd**
    
- **Fan-out/fan-in**
    
- **Cold data tier**
    
- **Backpressure**
    
- **Rate limiting**
    
- **Circuit breaker**
    

â€¦ communicates that you donâ€™t just know _concepts_ but also _operational realities_ of distributed systems â€” **how they behave under pressure**.

---

## ðŸ—£ï¸ Example Interview Conversation

**Interviewer:** What caching challenges have you seen in large-scale systems?  
**You:**

> â€œOne common challenge is the _hot key problem_, where a single cache key gets disproportionate traffic and overloads one Redis node. We solved this by adding key-level randomization and local in-memory caching. We also had a _hot API_ for our homepage that required aggressive CDN caching to handle spikes.â€

â†’ This shows awareness, scalability thinking, and production experience.

 
Letâ€™s now build a **theoretical, structured blog-style explanation** of these advanced _resilience and performance patterns_ that often come up in distributed systems interviews â€” such as **Cache Stampede, Thundering Herd, Backpressure, Circuit Breaker**, and a few more high-impact ones you can naturally mention like **Load Shedding, Bulkhead Isolation, Rate Limiting, Fail Fast**, and **Retry with Exponential Backoff**.

Each section below explains:

1. The **concept** (theory)
    
2. The **problem it solves**
    
3. The **common strategies** or **patterns** used
    
4. **Interview cross-questions** and how to answer them
    
5. A **real-world example** that makes your answer sound experienced
    

---

# ðŸ§© Advanced System Design Concepts for Scalability & Resilience

---

## âš™ï¸ 1. Cache Stampede (a.k.a. Dogpile Effect)

### ðŸ§  Concept:

A **cache stampede** occurs when a cache entry expires, and suddenly **multiple concurrent requests** try to fetch and recompute the same data â€” **overloading the database or backend**.

Itâ€™s like everyone rushing to refill the same cache entry at once.

### ðŸš¨ Problem It Solves:

Without protection, the system experiences a **sudden traffic spike** to the origin (DB, microservice, etc.), leading to latency or even cascading failures.

### ðŸ’¡ Common Mitigation Strategies:

- **Locking / Single Flight**: Only one thread recomputes the data; others wait.
    
- **Early Revalidation (Soft TTL)**: Refresh cache _before_ it expires.
    
- **Randomized TTLs**: Add jitter to cache expiry times.
    
- **Background Refresh**: Asynchronously update cache in the background.
    

### ðŸ’¬ Real-world Example:

> â€œWe faced a _cache stampede_ issue in Redis when our homepage cache expired, leading to thousands of DB hits in seconds. We implemented soft TTL and added random jitter to prevent synchronized expirations.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œHow is it different from a thundering herd?â€_  
    â†’ A cache stampede is specifically about **cache expiration**, while thundering herd is a **general traffic surge** to any resource.
    
- _â€œHow do you prevent it without adding latency?â€_  
    â†’ Use soft TTL or background refresh â€” refresh in advance.
    

---

## ðŸ˜ 2. Thundering Herd Problem

### ðŸ§  Concept:

A **thundering herd** happens when **many clients or threads** wake up simultaneously to process the same event or access a single shared resource â€” causing a massive load surge.

Itâ€™s a _synchronized spike_ problem, not necessarily cache-specific.

### ðŸš¨ Problem It Solves:

Avoids _resource contention_ and _spike overload_ on downstream services, databases, or locks.

### ðŸ’¡ Common Mitigation Strategies:

- **Request coalescing** (merge identical requests)
    
- **Exponential backoff + jitter** on retries
    
- **Distributed rate limiting**
    
- **Batching or queueing** incoming requests
    
- **Staggered wake-ups** (using randomized timers)
    

### ðŸ’¬ Real-world Example:

> â€œWe noticed a _thundering herd_ on our Kafka consumers when a topic became available â€” all consumers woke up simultaneously. We fixed it by adding jittered scheduling and batch consumption.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œWhatâ€™s the difference between thundering herd and cache stampede?â€_  
    â†’ Cache stampede is a special case of thundering herd.
    
- _â€œHow would you handle it in a distributed queue system?â€_  
    â†’ Use staggered polling intervals, exponential backoff, or leader election for coordination.
    

---

## ðŸ§µ 3. Backpressure

### ðŸ§  Concept:

**Backpressure** is a _flow control mechanism_ that helps systems **slow down producers** when **consumers are overwhelmed** â€” preventing queue buildup, OOM errors, or latency spikes.

Think of it like a pressure valve in a water pipe â€” it signals upstream systems to stop or slow data flow.

### ðŸš¨ Problem It Solves:

In event-driven or streaming systems, producers may emit messages faster than consumers can handle. Without backpressure, you get **queue overflow** or **system crashes**.

### ðŸ’¡ Common Mitigation Strategies:

- **Reactive Streams / Flow control signals** (e.g., in Spring WebFlux, Kafka, gRPC streaming)
    
- **Bounded queues**
    
- **Dropping or buffering strategies**
    
- **Load shedding when thresholds are reached**
    

### ðŸ’¬ Real-world Example:

> â€œWe implemented _backpressure_ in our reactive pipeline to avoid overloading the downstream payment service when the Kafka topic spiked.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œWhat happens if backpressure isnâ€™t implemented?â€_  
    â†’ Consumers get overwhelmed, causing latency, memory buildup, or service crashes.
    
- _â€œHow does backpressure differ from rate limiting?â€_  
    â†’ Rate limiting controls _external clients_; backpressure controls _internal system flow_.
    

---

## âš¡ 4. Circuit Breaker Pattern

### ðŸ§  Concept:

The **Circuit Breaker** is a resilience pattern that **prevents cascading failures** by _cutting off requests_ to a failing downstream service.

Itâ€™s inspired by electrical circuits â€” once too many failures occur, the breaker _opens_ and blocks further calls temporarily.

### âš™ï¸ States:

1. **Closed:** Normal operation.
    
2. **Open:** Block requests due to repeated failures.
    
3. **Half-Open:** Allow limited requests to test recovery.
    

### ðŸš¨ Problem It Solves:

Prevents overloading a service thatâ€™s already failing, and stops consuming resources on guaranteed failures.

### ðŸ’¡ Common Implementations:

- Netflix Hystrix (legacy)
    
- Resilience4j (modern Java library)
    
- Istio / Envoy sidecars for service mesh
    

### ðŸ’¬ Real-world Example:

> â€œWe applied a _circuit breaker_ around our user-service calls to prevent cascading failures when it went down. The breaker opened after 5 timeouts, and half-opened after 30 seconds.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œHow is it different from retry?â€_  
    â†’ Retry keeps trying; circuit breaker **stops** trying after a threshold.
    
- _â€œWhen do you reset the circuit breaker?â€_  
    â†’ After a cooldown period or during half-open state tests.
    
- _â€œWhat metrics decide opening the breaker?â€_  
    â†’ Failure rate, timeout rate, or latency thresholds.
    

---

## ðŸª£ 5. Bulkhead Pattern

### ðŸ§  Concept:

**Bulkhead isolation** means dividing a system into _isolated compartments_, so if one fails, it doesnâ€™t sink the entire system â€” just like watertight sections in a ship.

### ðŸš¨ Problem It Solves:

Prevents cascading failures by isolating resource usage between services or threads.

### ðŸ’¡ Common Mitigation Strategies:

- Separate **thread pools**, **connection pools**, or **service quotas**.
    
- Limit impact radius of failure.
    

### ðŸ’¬ Real-world Example:

> â€œWe used _bulkhead isolation_ by assigning dedicated thread pools per downstream dependency to prevent one slow API from starving others.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œHow does this differ from circuit breaker?â€_  
    â†’ Circuit breaker stops calling a failing service; bulkhead prevents one from impacting others.
    

---

## ðŸš§ 6. Rate Limiting

### ðŸ§  Concept:

**Rate limiting** restricts how many requests a client or service can make in a given time window.

### ðŸš¨ Problem It Solves:

Protects services from abuse, DDoS attacks, or unintentional client spikes.

### ðŸ’¡ Common Algorithms:

- Token Bucket
    
- Leaky Bucket
    
- Fixed Window / Sliding Window
    

### ðŸ’¬ Real-world Example:

> â€œWe applied _rate limiting_ using a token bucket algorithm at API Gateway level to prevent clients from overwhelming our backend.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œHow does it differ from backpressure?â€_  
    â†’ Rate limiting controls _incoming requests from outside_; backpressure controls _internal flow_.
    

---

## ðŸ’¥ 7. Load Shedding

### ðŸ§  Concept:

**Load shedding** is the deliberate dropping of requests when the system is near capacity â€” to keep response times stable and avoid total collapse.

### ðŸš¨ Problem It Solves:

Maintains system stability under extreme load.

### ðŸ’¡ Common Mitigation Strategies:

- Prioritize high-value requests.
    
- Drop non-essential background tasks.
    
- Return HTTP 503 with retry hints.
    

### ðŸ’¬ Real-world Example:

> â€œDuring peak load, we applied _load shedding_ by rejecting low-priority analytics events to maintain latency SLAs on user transactions.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œIsnâ€™t dropping requests bad?â€_  
    â†’ Sometimes itâ€™s better to fail fast for some users than to fail completely for all.
    

---

## â±ï¸ 8. Retry with Exponential Backoff & Jitter

### ðŸ§  Concept:

Instead of retrying failed requests immediately (which worsens overload), use **exponential backoff** â€” wait progressively longer before retrying.

**Jitter** adds randomness to prevent synchronized retries from many clients (which causes _thundering herd_).

### ðŸ’¬ Real-world Example:

> â€œWe used exponential backoff with jitter in our gRPC clients to avoid synchronized retry storms when the downstream service recovered.â€

### ðŸŽ¯ Interview Cross-Questions:

- _â€œWhy add jitter?â€_  
    â†’ To desynchronize retries â€” otherwise, everyone retries at the same time.
    

---

# ðŸ§± Summary Table

|Concept|Problem it Solves|Typical Layer|Key Strategy|
|---|---|---|---|
|Cache Stampede|Cache expiry overload|Caching Layer|Soft TTL, jitter|
|Thundering Herd|Sudden synchronized spikes|Any layer|Staggered wakeups|
|Backpressure|Producer > Consumer rate|Streaming|Flow control|
|Circuit Breaker|Cascading failure|Service-to-Service|Fail fast, cooldown|
|Bulkhead|Isolation of failures|Thread/resource layer|Separate pools|
|Rate Limiting|Client abuse control|Gateway/API|Token bucket|
|Load Shedding|Overload protection|Application|Drop low-priority load|
|Retry w/ Backoff|Retry storms|Client/Network|Exponential backoff + jitter|



This block builds on what weâ€™ve already covered (Cache Stampede, Thundering Herd, Backpressure, Circuit Breaker, Bulkhead, Rate Limiting, Load Shedding, Retry + Backoff).  
Now weâ€™ll cover additional **production-grade reliability, scalability, and observability patterns** â€” concepts senior engineers often bring up in design interviews or architecture reviews.

---

# âš¡ **Advanced System Design Concepts â€“ Block 2**

---

## ðŸ§© 1. **Idempotency**

### ðŸ§  Concept:

An **idempotent operation** is one that can be safely retried **multiple times** without changing the result beyond the initial application.

In distributed systems, retries are common due to transient failures â€” _idempotency ensures correctness_ even under duplicate requests.

### ðŸ’¡ Example:

- `PUT /users/123` with a full body is **idempotent** (same result no matter how many times itâ€™s called).
    
- `POST /users` (creating new resource) is **non-idempotent**, unless you use **idempotency keys**.
    

### âš™ï¸ Techniques:

- Use **unique request IDs (UUID or idempotency key)** to detect duplicates.
    
- Maintain a **request log table** to discard duplicate operations.
    

### ðŸ’¬ Interview Insight:

> â€œIn our payment microservice, we made transaction creation _idempotent_ using a unique idempotency key to avoid double charges on retries.â€

**Cross-question:**

- _How is it different from at-least-once vs exactly-once delivery?_  
    â†’ Idempotency is an application-level guarantee; those are message delivery semantics.
    

---

## âš–ï¸ 2. **Consistency Models**

### ðŸ§  Concept:

Distributed systems often sacrifice _immediate consistency_ to gain _availability and partition tolerance_ (CAP theorem).  
There are multiple **consistency models**, each balancing latency vs correctness.

### ðŸ’¡ Common Models:

- **Strong Consistency**: Always up-to-date (e.g., traditional RDBMS).
    
- **Eventual Consistency**: Updates propagate asynchronously (e.g., DynamoDB, Cassandra).
    
- **Read-Your-Writes**: Youâ€™ll always see your own recent writes.
    
- **Monotonic Reads**: Once youâ€™ve seen a value, youâ€™ll never see an older one.
    

### ðŸ’¬ Interview Insight:

> â€œWe chose _eventual consistency_ for user activity feeds since real-time strict consistency wasnâ€™t critical, but latency was.â€

**Cross-Question:**

- _When do you prefer strong consistency?_  
    â†’ For transactions like payments or inventory where correctness matters more than speed.
    

---

## ðŸ§® 3. **CAP Theorem & PACELC Theorem**

### ðŸ§  Concept:

- **CAP Theorem**: A distributed system can only guarantee two of three â€” **Consistency, Availability, Partition Tolerance**.
    
- **PACELC Theorem** extends it: _Even when thereâ€™s no partition_, you must choose between **Latency (L)** and **Consistency (C)**.
    

### ðŸ’¬ Interview Insight:

> â€œWe designed our system as AP â€” prioritizing availability over consistency during partitions, acceptable for analytics data.â€

**Cross-Question:**

- _What would you pick for a payment system?_  
    â†’ CP â€” Consistency + Partition tolerance, sacrificing availability.
    

---

## ðŸ“¦ 4. **Eventual Consistency & Conflict Resolution**

### ðŸ§  Concept:

In **eventually consistent systems**, replicas will converge over time. But if two replicas accept writes concurrently, **conflicts occur**.

### ðŸ’¡ Resolution Strategies:

- **Last Write Wins (LWW)**
    
- **Vector Clocks**
    
- **CRDTs (Conflict-Free Replicated Data Types)**
    

### ðŸ’¬ Interview Insight:

> â€œWe used CRDTs in our chat service to merge concurrent edits without conflict â€” it guarantees eventual convergence.â€

**Cross-Question:**

- _When would you prefer LWW over CRDT?_  
    â†’ When order matters more than intermediate states (like timestamps in logs).
    

---

## ðŸ•¸ï¸ 5. **Data Sharding & Hotspotting**

### ðŸ§  Concept:

**Sharding** splits large datasets horizontally into **shards** (smaller, distributed partitions).  
**Hotspotting** occurs when uneven data access overloads one shard (similar to _hot partition_).

### ðŸ’¡ Sharding Strategies:

- **Hash-based** (uniform distribution)
    
- **Range-based** (sorted order)
    
- **Directory-based** (lookup table)
    

### ðŸ’¬ Interview Insight:

> â€œWe implemented hash-based sharding to evenly distribute users across shards and avoid _hotspotting_.â€

**Cross-Question:**

- _How do you rebalance shards dynamically?_  
    â†’ Add new shards, migrate subsets, or use consistent hashing.
    

---

## ðŸ•°ï¸ 6. **Leader Election**

### ðŸ§  Concept:

In distributed coordination, **leader election** ensures that only one node performs a critical task at a time (avoiding race conditions).

### ðŸ’¡ Tools:

- **Zookeeper**, **etcd**, **Consul**, **Raft protocol**, or **Kubernetes controllers**.
    

### ðŸ’¬ Interview Insight:

> â€œWe used _Zookeeper_ for leader election to ensure only one scheduler performed batch job dispatching.â€

**Cross-Question:**

- _How does Raft ensure consensus?_  
    â†’ Through leader election, log replication, and majority quorum.
    

---

## ðŸ” 7. **Exactly-Once, At-Least-Once, At-Most-Once Semantics**

### ðŸ§  Concept:

Defines how many times a message might be processed in distributed systems.

|Delivery Type|Description|Use Case|
|---|---|---|
|**At Most Once**|Message may be lost but never duplicated|Non-critical logs|
|**At Least Once**|Guaranteed delivery but may duplicate|Payments, email|
|**Exactly Once**|Delivered once with no duplicates|Stream processing (Kafka + idempotent consumer)|

### ðŸ’¬ Interview Insight:

> â€œOur Kafka consumers ensured _exactly-once_ semantics by combining idempotent producers and transactional offsets.â€

**Cross-Question:**

- _How to implement exactly-once delivery?_  
    â†’ Use transactions and idempotency; pure exactly-once is theoretical across distributed systems.
    

---

## ðŸ§  8. **Distributed Locking**

### ðŸ§  Concept:

A mechanism to coordinate access to shared resources across nodes.

### ðŸ’¡ Implementations:

- **Redis RedLock**
    
- **Zookeeper ephemeral nodes**
    
- **Database-based advisory locks**
    

### ðŸ’¬ Interview Insight:

> â€œWe used _Redis-based distributed locks_ for synchronizing inventory updates across replicas.â€

**Cross-Question:**

- _How to ensure lock safety in Redis?_  
    â†’ Use `SETNX` with expiry; use RedLock algorithm for reliability.
    

---

## ðŸ§° 9. **Event-Driven Architecture & Message Ordering**

### ðŸ§  Concept:

In **event-driven systems**, services communicate asynchronously through **events**.  
The challenge is **message ordering**, **duplication**, and **reprocessing**.

### ðŸ’¡ Techniques:

- **Kafka partition key** ensures ordering per key.
    
- **Idempotent consumers** to handle retries.
    
- **Dead-letter queues (DLQ)** for failed messages.
    

### ðŸ’¬ Interview Insight:

> â€œWe used Kafkaâ€™s partition key to maintain ordering per user and a DLQ for failed messages to ensure no data loss.â€

**Cross-Question:**

- _How do you guarantee global ordering?_  
    â†’ Usually impractical â€” enforce local ordering within a partition key.
    

---

## ðŸ“Š 10. **Observability: Tracing, Logging, Metrics**

### ðŸ§  Concept:

**Observability** is the systemâ€™s ability to let you understand _why itâ€™s behaving a certain way_.  
It consists of:

- **Metrics** (numerical indicators like latency, QPS)
    
- **Logs** (event details)
    
- **Traces** (cross-service flow)
    

### ðŸ’¡ Tools:

- **Prometheus** (metrics)
    
- **Grafana** (visualization)
    
- **ELK Stack** (logs)
    
- **Jaeger / OpenTelemetry** (tracing)
    

### ðŸ’¬ Interview Insight:

> â€œWe implemented distributed tracing using _OpenTelemetry_ to identify latency bottlenecks across microservices.â€

**Cross-Question:**

- _Whatâ€™s the difference between monitoring and observability?_  
    â†’ Monitoring tells you _what_ is wrong; observability helps you find _why_ itâ€™s wrong.
    

---

## ðŸ§± 11. **Quorum & Consensus**

### ðŸ§  Concept:

In distributed databases, a **quorum** ensures that a minimum number of nodes must agree before confirming a write or read â€” guaranteeing consistency.

### ðŸ’¡ Formula:

For N nodes, require **W + R > N** to ensure overlap (e.g., Cassandra, DynamoDB).

### ðŸ’¬ Interview Insight:

> â€œWe used a quorum of 3 out of 5 replicas in Cassandra to maintain strong read-after-write consistency.â€

**Cross-Question:**

- _Why not always require all replicas?_  
    â†’ Because it increases latency and reduces availability.
    

---

## ðŸ§ 12. **Graceful Degradation**

### ðŸ§  Concept:

When a system is under partial failure or high load, it should _degrade gracefully_ instead of completely failing.

### ðŸ’¡ Examples:

- Disable non-critical features (e.g., recommendations)
    
- Show cached data or fallback pages
    
- Queue low-priority tasks
    

### ðŸ’¬ Interview Insight:

> â€œWe built _graceful degradation_ into our platform so even if the personalization service fails, users still see the base content.â€

**Cross-Question:**

- _How is it related to load shedding?_  
    â†’ Load shedding _drops_ requests; graceful degradation _simplifies responses_.
    

---

# ðŸ§± Summary Table (Block 2)

|Concept|Problem It Solves|Key Idea|
|---|---|---|
|Idempotency|Retry safety|Avoid duplicate side effects|
|Consistency Models|Data correctness|Strong vs eventual|
|CAP / PACELC|Trade-offs|Consistency vs availability/latency|
|Eventual Consistency|Conflict resolution|CRDTs, vector clocks|
|Sharding|Scalability|Horizontal data split|
|Leader Election|Coordination|One node executes critical task|
|Delivery Semantics|Message guarantees|At-most, at-least, exactly-once|
|Distributed Locking|Concurrency control|Shared resource safety|
|Event-driven|Async communication|Ordering + DLQ|
|Observability|Visibility|Metrics, logs, traces|
|Quorum|Consensus|R + W > N rule|
|Graceful Degradation|Partial resilience|Serve degraded responses|

---

