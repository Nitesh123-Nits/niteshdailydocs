![[Pasted image 20251122171427.png]]When choosing a **shard key** (in a sharded database like MongoDB, Cassandra, or any distributed data system), these **three principles** are the most critical. Letâ€™s break them down clearly:

---

### ğŸ”‘ 1. **High Cardinality**

**Meaning:**

- The shard key should have **many unique possible values**.
    
- This ensures that data can be distributed across all shards rather than clustering around a few.
    

**Why it matters:**

- Low-cardinality fields (like â€œcountryâ€ or â€œstatusâ€) cause **data skew** â€” one shard gets most of the writes, leading to performance bottlenecks.
    

**Example:**

|Field|Cardinality|Good Shard Key?|Why|
|---|---|---|---|
|`gender`|Low (2 values)|âŒ No|All data goes to only 2 shards|
|`userId`|High (millions of users)|âœ… Yes|Even spread across shards|

---

### âš–ï¸ 2. **Even Data Distribution**

**Meaning:**

- The shard key should spread data **uniformly across shards** to prevent **hotspots** (shards getting uneven traffic).
    

**How to achieve:**

- Use a **hashed** shard key if values are sequential (e.g., MongoDBâ€™s `hashed(userId)`).
    
- Combine multiple fields (compound shard key) like `region + userId` if you need some locality but still want good distribution.
    

**Example:**

- `timestamp` â†’ âŒ bad (monotonic, all new writes go to the latest shard)
    
- `hashed(userId)` â†’ âœ… good (randomized, balanced write load)
    

---

### ğŸ§­ 3. **Align With Query Patterns**

**Meaning:**

- The shard key must **match your most common query filters** to enable **targeted queries** (querying one shard instead of all).
    

**Why it matters:**

- If your shard key doesnâ€™t align with query filters, every query becomes a **scatter-gather** â€” sent to all shards â†’ poor performance.
    

**Example:**  
If your common queries are:

```js
db.orders.find({ userId: 123 })
```

Then `userId` (or `hashed(userId)`) is a perfect shard key.

If queries are like:

```js
db.orders.find({ region: 'US', orderDate: { $gt: ... } })
```

Then a **compound shard key** like `{ region, orderDate }` might be ideal.

---

### ğŸ§© Summary: Ideal Shard Key Checklist

|Principle|Description|Example|
|---|---|---|
|**High Cardinality**|Many unique values|`userId`, `email`, `UUID`|
|**Even Distribution**|Avoid hotspots or uneven load|`hashed(userId)`, `region + userId`|
|**Aligned with Queries**|Matches most frequent filters|If queries often use `userId`, use it as shard key|

---

### ğŸš« Common Mistakes

- Sharding on **timestamp** â†’ causes all writes to newest shard (hotspot).
    
- Sharding on **low-cardinality** field â†’ uneven data split.
    
- Shard key not used in queries â†’ scatter-gather overhead.
    

---

Would you like me to show a **few practical shard key examples** for systems like MongoDB, Cassandra, and Elasticsearch side by side?  
That way you can see how these principles apply in real design scenarios.

## Question
Letâ€™s go through **real-world shard key design examples** across three popular distributed databases â€” **MongoDB**, **Cassandra**, and **Elasticsearch** â€” to see how _high cardinality_, _even distribution_, and _query alignment_ work together.

---

## ğŸ§± 1. **MongoDB**

### âœ… Good Example: `hashed(userId)`

**Use Case:** Large multi-tenant app (e.g., e-commerce, social media).

|Aspect|Explanation|
|---|---|
|**High Cardinality**|Every userId is unique â€” millions of possible values.|
|**Even Distribution**|`hashed` key randomizes userIds â†’ avoids hotspots.|
|**Aligns with Queries**|Most queries filter by userId (e.g., `find({ userId: 123 })`).|

**Result:**  
âœ” Balanced writes and queries target only one shard.  
âš  Downside â€” you lose ordering by userId (hash breaks order).

---

### âš  Bad Example: `createdAt` (timestamp)

|Aspect|Issue|
|---|---|
|**Cardinality**|High â€” timestamps are unique.|
|**Distribution**|âŒ Bad â€” writes always go to the most recent shard.|
|**Query Alignment**|âŒ Often causes hotspot writes.|

**Result:**  
Uneven write load, one shard overloaded (the latest).

---

### âœ… Alternative: Compound Shard Key `{ region, hashed(userId) }`

|Aspect|Benefit|
|---|---|
|**High Cardinality**|Region + hashed userId gives large unique key space.|
|**Even Distribution**|Hashing ensures balance even within each region.|
|**Query Alignment**|Matches queries filtering by region and user.|

**Result:**  
Perfect for region-based apps like food delivery or social platforms.

---

## ğŸª´ 2. **Cassandra**

Cassandra partitions data based on the **partition key** (acts like shard key).

### âœ… Good Example: `(user_id)`

**Use Case:** Accessing user data or timeline.

|Aspect|Explanation|
|---|---|
|**High Cardinality**|Every user_id is unique.|
|**Even Distribution**|Partition key is hashed internally â†’ auto-balanced.|
|**Query Alignment**|Queries like `SELECT * FROM posts WHERE user_id = ?` hit one partition.|

**Result:**  
Fast lookups, evenly distributed partitions.

---

### âš  Bad Example: `(country)`

|Aspect|Issue|
|---|---|
|**Cardinality**|Low â€” few countries.|
|**Distribution**|âŒ Skewed â€” US/India shards get overloaded.|
|**Query Alignment**|Queries not selective â€” read many partitions.|

**Result:**  
Hot partitions and unbalanced storage.

---

### âœ… Better: Compound Partition Key `(country, user_id)`

|Aspect|Benefit|
|---|---|
|**High Cardinality**|Combination increases uniqueness.|
|**Even Distribution**|User_id spreads data inside each country.|
|**Query Alignment**|Enables country-specific queries too.|

---

## ğŸ” 3. **Elasticsearch**

Elasticsearch distributes data using **index shards** and routing keys.

### âœ… Good Example: `userId` (as routing key)

**Use Case:** Querying logs or posts per user.

|Aspect|Explanation|
|---|---|
|**High Cardinality**|Many users â†’ many routing values.|
|**Even Distribution**|User IDs distributed evenly across shards.|
|**Query Alignment**|You can route queries directly to that userâ€™s shard (`routing=userId`).|

**Result:**  
Low latency, reduced cluster load.

---

### âš  Bad Example: `region`

|Aspect|Issue|
|---|---|
|**Cardinality**|Low.|
|**Distribution**|Some shards overloaded with popular regions.|
|**Query Alignment**|If your queries arenâ€™t by region, routing fails.|

---

## âš™ï¸ Comparison Table

|Database|Concept|Good Example|Bad Example|Key Balancing Method|
|---|---|---|---|---|
|**MongoDB**|Shard Key|`hashed(userId)`|`createdAt`|Hash-based sharding|
|**Cassandra**|Partition Key|`(user_id)`|`(country)`|Murmur3 hashing|
|**Elasticsearch**|Routing Key|`userId`|`region`|Routing hash function|

---

### ğŸ§­ General Rule of Thumb

|Goal|Guideline|
|---|---|
|**Avoid hotspots**|Donâ€™t use monotonically increasing keys (timestamps, IDs).|
|**Keep queries targeted**|Choose shard key that appears in your most common query filter.|
|**Enable scalability**|Prefer high-cardinality fields or compound keys.|
|**Balance writes and reads**|Use hashing when ordering isnâ€™t important.|

---
## Question

**Different types of sharding** is fundamental for system design interviews and real-world architecture.

Letâ€™s break this down **clearly, deeply, and visually structured** ğŸ‘‡

---

## ğŸ§© **What Is Sharding?**

**Sharding** = breaking a large dataset into smaller, faster, more manageable parts called **shards**, which are distributed across multiple servers (nodes).

Itâ€™s mainly used to:

- Scale horizontally
    
- Improve performance and availability
    
- Distribute load and storage
    

---

## ğŸ§± **Types of Sharding**

There are **four major types** of sharding you should know:

1. **Range-based Sharding**
    
2. **Hash-based Sharding**
    
3. **Directory / Lookup-based Sharding**
    
4. **Composite / Hybrid Sharding**
    

Letâ€™s explore each.

---

### 1ï¸âƒ£ **Range-Based Sharding**

**How it works:**  
Data is divided into shards based on value **ranges** of the shard key.

**Example:**

|Range|Shard|
|---|---|
|`userId 1â€“1,000,000`|Shard A|
|`userId 1,000,001â€“2,000,000`|Shard B|
|`userId 2,000,001â€“3,000,000`|Shard C|

**âœ… Pros**

- Simple to implement and understand
    
- Good for **range queries** (e.g., date ranges, sequential IDs)
    
- Easy to predict where data lives
    

**âŒ Cons**

- **Hotspot risk:** all new data (e.g., new users or latest timestamps) go to one shard
    
- Rebalancing can be costly if ranges are uneven
    

**Used in:**

- MySQL (manual range sharding)
    
- MongoDB (range shard keys)
    
- Elasticsearch (time-based indices)
    

---

### 2ï¸âƒ£ **Hash-Based Sharding**

**How it works:**  
Apply a **hash function** to the shard key, then assign the data to shards based on the hash output.

**Example:**  
`hash(userId) % num_shards`

|userId|Hash|Shard|
|---|---|---|
|101|1|Shard A|
|102|2|Shard B|
|103|0|Shard C|

**âœ… Pros**

- **Even data distribution** (reduces hotspots)
    
- Great for **random access patterns**
    
- Ideal when queries are key-based (e.g., by userId, productId)
    

**âŒ Cons**

- **No range queries** (hash breaks order)
    
- Harder to rebalance if shard count changes (consistent hashing helps)
    

**Used in:**

- MongoDB (`hashed` shard key)
    
- Cassandra (partition key hashing)
    
- DynamoDB (partition hash key)
    

---

### 3ï¸âƒ£ **Directory-Based (Lookup-Based) Sharding**

**How it works:**  
A **mapping service (directory)** stores which shard each key belongs to.

**Example:**

|userId|Shard|
|---|---|
|101|Shard A|
|102|Shard B|
|103|Shard C|

When a query arrives:

- Directory lookup â†’ find correct shard â†’ forward the query.
    

**âœ… Pros**

- Very **flexible** â€” any sharding rule possible
    
- Easy to **reshard or rebalance** dynamically
    
- Can mix rules (region, user type, etc.)
    

**âŒ Cons**

- **Directory is a single point of failure** (unless replicated)
    
- Extra lookup step adds latency
    
- Directory must stay consistent
    

**Used in:**

- Facebookâ€™s TAO (social graph store)
    
- Custom enterprise sharding routers
    

---

### 4ï¸âƒ£ **Composite / Hybrid Sharding**

**How it works:**  
Combines **multiple sharding strategies** â€” for example:

- **Hash + Range**
    
- **Range + Directory**
    
- **Region + Hash**
    

**Example:**  
Shard first by `region`, then within region by `hashed(userId)`:

|Region|Hash(userId)|Shard|
|---|---|---|
|US|0â€“4|Shard A|
|US|5â€“9|Shard B|
|IN|0â€“4|Shard C|
|IN|5â€“9|Shard D|

**âœ… Pros**

- Very **flexible** and **balanced**
    
- Supports both **regional locality** and **load distribution**
    
- Minimizes cross-region latency
    

**âŒ Cons**

- More complex to design and maintain
    
- Rebalancing logic gets complicated
    

**Used in:**

- Global-scale systems (Netflix, Uber, Amazon)
    
- MongoDB compound shard keys
    

---

## âš™ï¸ **Comparison Table**

|Type|How It Works|Pros|Cons|Use Case|
|---|---|---|---|---|
|**Range-Based**|Split by value ranges|Great for range queries|Hotspots on high/low ends|Time-series, ordered data|
|**Hash-Based**|Hash of key â†’ shard|Even distribution|No range queries|User-based systems|
|**Directory-Based**|Lookup table maps keys to shards|Highly flexible|Directory overhead|Dynamic routing systems|
|**Composite (Hybrid)**|Combine strategies|Balanced + flexible|Complex logic|Multi-region, large-scale apps|

---

## ğŸ§  Bonus: **Consistent Hashing**

A **variant of hash sharding** used in systems like **Cassandra**, **DynamoDB**, **Kafka**, and **Redis Cluster**.

**Idea:**  
Instead of `hash(key) % N`, use a **hash ring** â€” when adding/removing a shard, only a small subset of keys move.

**âœ… Benefit:**  
Highly scalable and avoids full data reshuffling.

---

## ğŸ§­ When to Use Which

|Scenario|Best Sharding Type|
|---|---|
|Time-series or logs|Range-based|
|User-based systems|Hash-based|
|Complex routing or dynamic workloads|Directory-based|
|Global apps (e.g., region + user)|Composite/Hybrid|
|Streaming systems (Kafka, Redis)|Consistent hashing|

---
